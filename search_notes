Making search more efficient:

Most likely, we will need to improve the indexing.  Right now, the algorithm
is O(n).  However, as data grows, this quickly becomes untenable.  More 
efficient means are needed to reduce this time in most practical cases.

Maybe have a network of keywords and blocks attached to that keyword.  This
solution may actually be fairly ideal, especially as the number of blocks
increases beyond the number of common words, which is fairly fixed.

So, let us create a data-structure, maybe within mongo.  This data structure
will store a dict of keywords and blocks that have that keyword.  The
data structure is periodicially updated in batches, such that for each block,
it is attached to the associated set of the keyword.  If we use this, then
it may not be so important to distinguish between blocks and posts.  Instead,
if needed, an entry within the database will be of the form id : {type : post}.
Furthermore, within this database, the entries can roughly be configured as
an order tree based on frequency.

The other main issue is how to make this object persistent across files,
servers, and so on.  How efficient would it be to update it every so often?
Maybe the object can be updated in the background and copied to the foreground
to be used.

